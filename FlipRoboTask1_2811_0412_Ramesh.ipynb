{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354ab4ee",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment-1. \n",
    "# Project: Internship 34\n",
    "<!-- Deadline for the submission of the assignment is Sunday, 04-12-2022, 11:59 PM. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badeb1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Navigation menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Personal tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Namespaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Views</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Navigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Print/export</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In other projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Header tags\n",
       "0                       Main Page\n",
       "1            Welcome to Wikipedia\n",
       "2   From today's featured article\n",
       "3                Did you know ...\n",
       "4                     In the news\n",
       "5                     On this day\n",
       "6        Today's featured picture\n",
       "7        Other areas of Wikipedia\n",
       "8     Wikipedia's sister projects\n",
       "9             Wikipedia languages\n",
       "10                Navigation menu\n",
       "11                 Personal tools\n",
       "12                     Namespaces\n",
       "13                          Views\n",
       "14                     Navigation\n",
       "15                     Contribute\n",
       "16                          Tools\n",
       "17                   Print/export\n",
       "18              In other projects\n",
       "19                      Languages"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Write a python program to display all the header tags from wikipedia.org.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def wikipedia_HeaderTag(url):\n",
    "    #Web scraping a wikipedia article\n",
    "    #Wikiurl ='https://en.wikipedia.org/wiki/Main_Page'\n",
    "    Wikiurl = url\n",
    "    wkreq = requests.get(Wikiurl)\n",
    "    Headersoup = BeautifulSoup(wkreq.text, 'lxml')\n",
    "    # List of all header tags\n",
    "    header_tags = [\"h1\", \"h2\", \"h3\"]\n",
    "    #\"List of all h1, h2, h3 \n",
    "    htags=[]\n",
    "    for htag in Headersoup.find_all(header_tags):\n",
    "              #print(htag.name + ' == ' + htag.text.strip())\n",
    "                ht=htag.text.strip()\n",
    "                htags.append(ht)\n",
    "    df = pd.DataFrame({'Header tags':htags})\n",
    "    display(df)\n",
    "\n",
    "wikipedia_HeaderTag(\"https://en.wikipedia.org/wiki/Main_Page\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586d8a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Lawrence of Arabia</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Movie_Name  Rating Year of Release\n",
       "0            The Shawshank Redemption     9.2            1994\n",
       "1                       The Godfather     9.2            1972\n",
       "2                     The Dark Knight     9.0            2008\n",
       "3               The Godfather Part II     9.0            1974\n",
       "4                        12 Angry Men     9.0            1957\n",
       "..                                ...     ...             ...\n",
       "95                       Citizen Kane     8.3            1941\n",
       "96  M - Eine Stadt sucht einen Mörder     8.3            1931\n",
       "97                 Lawrence of Arabia     8.3            1962\n",
       "98                 North by Northwest     8.2            1959\n",
       "99                            Vertigo     8.2            1958\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "#and make data frame\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# IMDB's Top rated movies\n",
    "def imdbtop100_data(IMDBurl):\n",
    "    url = IMDBurl\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    movies = soup.select('td.titleColumn')\n",
    "    ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "    imdb_list = []\n",
    "    # PLace each item upto 100 top movies into dictionary (data), then put  into a list (imdb_list)\n",
    "    for index in range(0, 100):\n",
    "        # Seperate movie into:'title', 'year'\n",
    "        movie_string = movies[index].get_text()\n",
    "        movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "        movie_title = movie[len(str(index))+1:-7]\n",
    "        year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\n",
    "        data = {\"Movie_Name\": movie_title,\n",
    "                \"Rating\": round(float(ratings[index]),1),\n",
    "                \"Year of Release\": year,\n",
    "                }\n",
    "        imdb_list.append(data)\n",
    "\n",
    "    df = pd.DataFrame(imdb_list)\n",
    "    return df\n",
    "\n",
    "imdbtop100_data(\"http://www.imdb.com/chart/top\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b114d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Kaakkaa Muttai</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Theeran Adhigaaram Ondru</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Movie_Name  Rating Year of Release\n",
       "0   Ramayana: The Legend of Prince Rama     8.5            1993\n",
       "1            Rocketry: The Nambi Effect     8.4            2022\n",
       "2                           777 Charlie     8.4            2022\n",
       "3                               Golmaal     8.4            1979\n",
       "4                               Nayakan     8.4            1987\n",
       "..                                  ...     ...             ...\n",
       "95                       Kaakkaa Muttai     8.0            2014\n",
       "96                          Ustad Hotel     8.0            2012\n",
       "97             Theeran Adhigaaram Ondru     8.0            2017\n",
       "98                               Angoor     8.0            1982\n",
       "99                      Rang De Basanti     8.0            2006\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Download IMDB's Top rated 100 Indian movies\n",
    "def imdbtop100indianMovies_data(IMDBUrl):\n",
    "    #url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "    url = IMDBUrl\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    movies = soup.select('td.titleColumn')\n",
    "    ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "    imdb_list = []\n",
    "    # PLace each item upto 100 top movies into dictionary (data), then put  into a list (imdb_list)\n",
    "    for index in range(0, 100):\n",
    "        # Seperate movie into:'title', 'year'\n",
    "        movie_string = movies[index].get_text()\n",
    "        movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "        movie_title = movie[len(str(index))+1:-7]\n",
    "        year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\n",
    "        data = {\"Movie_Name\": movie_title,\n",
    "                \"Rating\": round(float(ratings[index]),1),\n",
    "                \"Year of Release\": year,\n",
    "                }\n",
    "        imdb_list.append(data)\n",
    "    \n",
    "    df = pd.DataFrame(imdb_list)\n",
    "    return df\n",
    "\n",
    "imdbtop100indianMovies_data(\"https://www.imdb.com/india/top-rated-indian-movies/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8558c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PresidentiaL List</th>\n",
       "      <th>Term of office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PresidentiaL List  \\\n",
       "0           Shri Ram Nath Kovind   \n",
       "1          Shri Pranab Mukherjee   \n",
       "2   Smt Pratibha Devisingh Patil   \n",
       "3         DR. A.P.J. Abdul Kalam   \n",
       "4           Shri K. R. Narayanan   \n",
       "5        Dr Shankar Dayal Sharma   \n",
       "6            Shri R Venkataraman   \n",
       "7               Giani Zail Singh   \n",
       "8      Shri Neelam Sanjiva Reddy   \n",
       "9       Dr. Fakhruddin Ali Ahmed   \n",
       "10  Shri Varahagiri Venkata Giri   \n",
       "11              Dr. Zakir Husain   \n",
       "12  Dr. Sarvepalli Radhakrishnan   \n",
       "13           Dr. Rajendra Prasad   \n",
       "\n",
       "                                       Term of office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) \n",
    "#from https://presidentofindia.nic.in/former-presidents.htm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def president_data(presurl):\n",
    "    url=requests.get(presurl)\n",
    "    soup=BeautifulSoup(url.content)\n",
    "    Term=[]\n",
    "    for i in soup.find_all('p'):\n",
    "        Term.append(i.text)\n",
    "    #print(Term)    \n",
    "    result_3 = [item.split(':') for item in Term]\n",
    "    result_4 = [item for l in result_3 for item in l]\n",
    "    # Remove word containing string in the list\n",
    "    result_5 = list(filter(lambda a: a != 'Term of Office', result_4))\n",
    "    # Remove words containing list characters\n",
    "    # using list comprehension + all()\n",
    "    from itertools import groupby\n",
    "    # initializing list\n",
    "    test_list = result_5\n",
    "    # initializing char list\n",
    "    char_list = ['http','https','//','\\n','Copyright','Page','PM\\n']\n",
    "\n",
    "    # Remove words containing list characters\n",
    "    # using list comprehension + all()\n",
    "    res = [ele for ele in test_list if all(ch not in ele for ch in char_list)]\n",
    "    # printing result\n",
    "    #print (\"The filtered strings are : \" + str(res))\n",
    "    Detail=[]\n",
    "    for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "        ii= i.text.split('(')[0].strip()\n",
    "        Detail.append(ii)\n",
    "        Detail\n",
    "\n",
    "    #print(Detail)\n",
    "    TermsFinal=[]\n",
    "    #get the terms of office which doesnt include the last row\n",
    "    for index in range(0, len(res)-1):\n",
    "        tt = res[index]\n",
    "        TermsFinal.append(tt)\n",
    "        TermsFinal\n",
    "        #print(TermsFinal)\n",
    "    data = {'PresidentiaL List':Detail,'Term of office':TermsFinal}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "president_data(\"https://presidentofindia.nic.in/former-presidents.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d385507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New Zealand\\nNZ</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>England\\nENG</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Australia\\nAUS</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>India\\nIND</td>\n",
       "      <td>35</td>\n",
       "      <td>3,866</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Pakistan\\nPAK</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>South Africa\\nSA</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh\\nBAN</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sri Lanka\\nSL</td>\n",
       "      <td>30</td>\n",
       "      <td>2,677</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Afghanistan\\nAFG</td>\n",
       "      <td>19</td>\n",
       "      <td>1,380</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>West Indies\\nWI</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pos              Team Matches Points Rating\n",
       "1    1   New Zealand\\nNZ      23  2,670    116\n",
       "2    2      England\\nENG      30  3,400    113\n",
       "3    3    Australia\\nAUS      32  3,572    112\n",
       "4    4        India\\nIND      35  3,866    110\n",
       "5    5     Pakistan\\nPAK      22  2,354    107\n",
       "6    6  South Africa\\nSA      24  2,392    100\n",
       "7    7   Bangladesh\\nBAN      30  2,753     92\n",
       "8    8     Sri Lanka\\nSL      30  2,677     89\n",
       "9    9  Afghanistan\\nAFG      19  1,380     73\n",
       "10  10   West Indies\\nWI      41  2,902     71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "      <th>CareerBestRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1                        (0)</td>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "      <td>898 v West Indies, 10/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2                                (0)</td>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "      <td>815 v West Indies, 12/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3                                (0)</td>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "      <td>796 v England, 19/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4                                (0)</td>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "      <td>813 v Sri Lanka, 10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5                                (0)</td>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "      <td>880 v Pakistan, 26/01/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6                                (0)</td>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "      <td>752 v Pakistan, 22/01/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7                                (2)This playe...</td>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "      <td>796 v India, 26/03/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8                                (1)This playe...</td>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "      <td>911 v England, 12/07/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9                                (1)This playe...</td>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>704</td>\n",
       "      <td>885 v Sri Lanka, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10                                (0)</td>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "      <td>799 v India, 09/07/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Rank                 Player  \\\n",
       "1                        1                        (0)             Babar Azam   \n",
       "2                2                                (0)            Imam-ul-Haq   \n",
       "3                3                                (0)  Rassie van der Dussen   \n",
       "4                4                                (0)        Quinton de Kock   \n",
       "5                5                                (0)           David Warner   \n",
       "6                6                                (0)            Steve Smith   \n",
       "7   7                                (2)This playe...         Jonny Bairstow   \n",
       "8   8                                (1)This playe...            Virat Kohli   \n",
       "9   9                                (1)This playe...           Rohit Sharma   \n",
       "10              10                                (0)        Kane Williamson   \n",
       "\n",
       "   Team Rating               CareerBestRating  \n",
       "1   PAK    890  898 v West Indies, 10/06/2022  \n",
       "2   PAK    779  815 v West Indies, 12/06/2022  \n",
       "3    SA    766      796 v England, 19/07/2022  \n",
       "4    SA    759    813 v Sri Lanka, 10/03/2019  \n",
       "5   AUS    747     880 v Pakistan, 26/01/2017  \n",
       "6   AUS    719     752 v Pakistan, 22/01/2017  \n",
       "7   ENG    710        796 v India, 26/03/2021  \n",
       "8   IND    707      911 v England, 12/07/2018  \n",
       "9   IND    704    885 v Sri Lanka, 06/07/2019  \n",
       "10   NZ    701        799 v India, 09/07/2019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI bowlers along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "      <th>CareerBestRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1                        (0)</td>\n",
       "      <td>Pat Cummins</td>\n",
       "      <td>AUS</td>\n",
       "      <td>891</td>\n",
       "      <td>914 v England, 18/08/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2                                (0)</td>\n",
       "      <td>Ravichandran Ashwin</td>\n",
       "      <td>IND</td>\n",
       "      <td>842</td>\n",
       "      <td>904 v England, 12/12/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3                                (0)</td>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>828</td>\n",
       "      <td>836 v Sri Lanka, 20/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4                                (0)</td>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>828</td>\n",
       "      <td>835 v West Indies, 02/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5                                (0)</td>\n",
       "      <td>James Anderson</td>\n",
       "      <td>ENG</td>\n",
       "      <td>825</td>\n",
       "      <td>903 v India, 13/08/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6                                (0)</td>\n",
       "      <td>Kagiso Rabada</td>\n",
       "      <td>SA</td>\n",
       "      <td>799</td>\n",
       "      <td>902 v Australia, 12/03/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7                                (0)</td>\n",
       "      <td>Kyle Jamieson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>788</td>\n",
       "      <td>836 v England, 06/06/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8                                (0)</td>\n",
       "      <td>Kemar Roach</td>\n",
       "      <td>WI</td>\n",
       "      <td>756</td>\n",
       "      <td>780 v India, 02/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9                                (0)</td>\n",
       "      <td>Neil Wagner</td>\n",
       "      <td>NZ</td>\n",
       "      <td>747</td>\n",
       "      <td>859 v Australia, 30/12/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10                                (0)</td>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>739</td>\n",
       "      <td>806 v New Zealand, 16/12/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Rank               Player Team Rating  \\\n",
       "1            1                        (0)          Pat Cummins  AUS    891   \n",
       "2    2                                (0)  Ravichandran Ashwin  IND    842   \n",
       "3    3                                (0)       Shaheen Afridi  PAK    828   \n",
       "4    4                                (0)       Jasprit Bumrah  IND    828   \n",
       "5    5                                (0)       James Anderson  ENG    825   \n",
       "6    6                                (0)        Kagiso Rabada   SA    799   \n",
       "7    7                                (0)        Kyle Jamieson   NZ    788   \n",
       "8    8                                (0)          Kemar Roach   WI    756   \n",
       "9    9                                (0)          Neil Wagner   NZ    747   \n",
       "10  10                                (0)       Mitchell Starc  AUS    739   \n",
       "\n",
       "                 CareerBestRating  \n",
       "1       914 v England, 18/08/2019  \n",
       "2       904 v England, 12/12/2016  \n",
       "3     836 v Sri Lanka, 20/07/2022  \n",
       "4   835 v West Indies, 02/09/2019  \n",
       "5         903 v India, 13/08/2018  \n",
       "6     902 v Australia, 12/03/2018  \n",
       "7       836 v England, 06/06/2022  \n",
       "8         780 v India, 02/09/2019  \n",
       "9     859 v Australia, 30/12/2020  \n",
       "10  806 v New Zealand, 16/12/2019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "# c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "    \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import threading\n",
    "\n",
    "def ODIMen_10data(ODImencricurl):\n",
    "    ODImenurl=ODImencricurl\n",
    "    data = requests.get(ODImenurl).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    MenCricData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        MenCricData.append(cols)\n",
    "    df = pd.DataFrame(MenCricData)\n",
    "    df.columns =['Pos', 'Team', 'Matches', 'Points','Rating']\n",
    "    df = df.iloc[1:11]\n",
    "    print(\"Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\")\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def ODIMenbatting_10data(menBatcricurl):\n",
    "    MenbatUrl = menBatcricurl\n",
    "    data = requests.get(MenbatUrl).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    MenCricData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        MenCricData.append(cols)\n",
    "    df = pd.DataFrame(MenCricData)\n",
    "    df.columns =['Rank', 'Player', 'Team', 'Rating','CareerBestRating']\n",
    "    df['Rank'] = df['Rank'].str.replace('\\n',\"\")\n",
    "    df = df.iloc[1:11]\n",
    "    print(\"Top 10 ODI Batsmen along with the records of their team and rating.\")\n",
    "    display(df)\n",
    "    \n",
    "def ODImenBowler_10data(cricurl):\n",
    "    menbowler = cricurl\n",
    "    data = requests.get(menbowler).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    MenCricData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        MenCricData.append(cols)\n",
    "    df = pd.DataFrame(MenCricData)\n",
    "    df.columns =['Rank', 'Player', 'Team', 'Rating','CareerBestRating']\n",
    "    df['Rank'] = df['Rank'].str.replace('\\n',\"\")\n",
    "    df = df.iloc[1:11]\n",
    "    print(\"Top 10 ODI bowlers along with the records of their team and rating.\")\n",
    "    display(df)\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    t1 = threading.Thread(target = ODIMen_10data, args=('https://www.icc-cricket.com/rankings/mens/team-rankings/odi',))\n",
    "    t2 = threading.Thread(target = ODIMenbatting_10data, args=('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting',))\n",
    "    t3 = threading.Thread(target = ODImenBowler_10data, args=('https://www.icc-cricket.com/rankings/mens/player-rankings/test/bowling',))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3872744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia\\nAUS</td>\n",
       "      <td>18</td>\n",
       "      <td>3,061</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>South Africa\\nSA</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>England\\nENG</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>India\\nIND</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>New Zealand\\nNZ</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>West Indies\\nWI</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh\\nBAN</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Thailand\\nTHA</td>\n",
       "      <td>8</td>\n",
       "      <td>572</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Pakistan\\nPAK</td>\n",
       "      <td>24</td>\n",
       "      <td>1,519</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Sri Lanka\\nSL</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pos              Team Matches Points Rating\n",
       "1    1    Australia\\nAUS      18  3,061    170\n",
       "2    2  South Africa\\nSA      26  3,098    119\n",
       "3    3      England\\nENG      25  2,904    116\n",
       "4    4        India\\nIND      27  2,820    104\n",
       "5    5   New Zealand\\nNZ      24  2,425    101\n",
       "6    6   West Indies\\nWI      24  2,334     97\n",
       "7    7   Bangladesh\\nBAN      12    932     78\n",
       "8    8     Thailand\\nTHA       8    572     72\n",
       "9    9     Pakistan\\nPAK      24  1,519     63\n",
       "10  10     Sri Lanka\\nSL       8    353     44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI all-rounder along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "      <th>CareerBestRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1                        (0)</td>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>380</td>\n",
       "      <td>380 v New Zealand, 25/09/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2                                (0)</td>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "      <td>548 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3                                (0)</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "      <td>395 v South Africa, 11/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4                                (0)</td>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "      <td>356 v West Indies, 25/09/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5                                (0)</td>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "      <td>419 v West Indies, 10/09/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6                                (0)</td>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "      <td>397 v South Africa, 09/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7                                (0)</td>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "      <td>279 v West Indies, 30/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8                                (0)</td>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "      <td>308 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9                                (0)</td>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "      <td>308 v Australia, 02/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10                                (0)</td>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "      <td>296 v Australia, 03/02/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Rank            Player Team Rating  \\\n",
       "1            1                        (0)   Hayley Matthews   WI    380   \n",
       "2    2                                (0)      Ellyse Perry  AUS    374   \n",
       "3    3                                (0)    Natalie Sciver  ENG    357   \n",
       "4    4                                (0)       Amelia Kerr   NZ    356   \n",
       "5    5                                (0)    Marizanne Kapp   SA    349   \n",
       "6    6                                (0)     Deepti Sharma  IND    322   \n",
       "7    7                                (0)  Ashleigh Gardner  AUS    270   \n",
       "8    8                                (0)     Jess Jonassen  AUS    246   \n",
       "9    9                                (0)    Jhulan Goswami  IND    214   \n",
       "10  10                                (0)   Katherine Brunt  ENG    207   \n",
       "\n",
       "                  CareerBestRating  \n",
       "1    380 v New Zealand, 25/09/2022  \n",
       "2    548 v West Indies, 11/09/2019  \n",
       "3   395 v South Africa, 11/07/2022  \n",
       "4    356 v West Indies, 25/09/2022  \n",
       "5    419 v West Indies, 10/09/2021  \n",
       "6   397 v South Africa, 09/10/2019  \n",
       "7    279 v West Indies, 30/03/2022  \n",
       "8    308 v West Indies, 11/09/2019  \n",
       "9      308 v Australia, 02/02/2016  \n",
       "10     296 v Australia, 03/02/2022  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI Batting players along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "      <th>CareerBestRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1                        (0)</td>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "      <td>785 v England, 03/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2                                (0)</td>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "      <td>748 v England, 03/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3                                (0)</td>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "      <td>741 v Australia, 22/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4                                (0)</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "      <td>755 v South Africa, 15/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5                                (0)</td>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "      <td>731 v England, 21/09/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6                                (0)</td>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "      <td>797 v England, 28/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7                                (0)</td>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "      <td>834 v New Zealand, 24/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8                                (0)</td>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "      <td>713 v West Indies, 15/03/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9                                (0)</td>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "      <td>756 v Australia, 02/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10                                (0)</td>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "      <td>691 v South Africa, 14/02/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Rank               Player Team Rating  \\\n",
       "1            1                        (0)         Alyssa Healy  AUS    785   \n",
       "2    2                                (0)          Beth Mooney  AUS    749   \n",
       "3    3                                (0)      Laura Wolvaardt   SA    732   \n",
       "4    4                                (0)       Natalie Sciver  ENG    725   \n",
       "5    5                                (0)     Harmanpreet Kaur  IND    716   \n",
       "6    6                                (0)      Smriti Mandhana  IND    714   \n",
       "7    7                                (0)          Meg Lanning  AUS    710   \n",
       "8    8                                (0)       Rachael Haynes  AUS    701   \n",
       "9    9                                (0)    Amy Satterthwaite   NZ    661   \n",
       "10  10                                (0)  Chamari Athapaththu   SL    655   \n",
       "\n",
       "                  CareerBestRating  \n",
       "1        785 v England, 03/04/2022  \n",
       "2        748 v England, 03/04/2022  \n",
       "3      741 v Australia, 22/03/2022  \n",
       "4   755 v South Africa, 15/07/2022  \n",
       "5        731 v England, 21/09/2022  \n",
       "6        797 v England, 28/02/2019  \n",
       "7    834 v New Zealand, 24/02/2016  \n",
       "8    713 v West Indies, 15/03/2022  \n",
       "9      756 v Australia, 02/03/2017  \n",
       "10  691 v South Africa, 14/02/2019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODIWomen_10data(ODIWomenCricurl):\n",
    "    # The test data\n",
    "    ODIWomenurl=ODIWomenCricurl\n",
    "    response = requests.get(ODIWomenCricurl)\n",
    "    data = requests.get(ODIWomenurl).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    WomenCricData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        WomenCricData.append(cols)\n",
    "    df = pd.DataFrame(WomenCricData)\n",
    "    df.columns =['Pos', 'Team', 'Matches', 'Points','Rating']\n",
    "    df = df.iloc[1:11]\n",
    "    print(\" Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\")\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def ODIWomenBatting_10data(WomenBatcricurl):\n",
    "    WomenBaturl = WomenBatcricurl\n",
    "    data = requests.get(WomenBaturl).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    WomenCricBatData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        WomenCricBatData.append(cols)\n",
    "    df = pd.DataFrame(WomenCricBatData)\n",
    "    df.columns =['Rank', 'Player', 'Team', 'Rating','CareerBestRating']\n",
    "    df['Rank'] = df['Rank'].str.replace('\\n',\"\")\n",
    "    df = df.iloc[1:11]\n",
    "    print(\"Top 10 women’s ODI Batting players along with the records of their team and rating.\")\n",
    "    display(df)\n",
    "   \n",
    "def ODIWomenAllrounder_10data(WomenAllroundcricurl):\n",
    "    Womenallrounder = WomenAllroundcricurl\n",
    "    data = requests.get(Womenallrounder).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    WomenAllroundData = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        WomenAllroundData.append(cols)\n",
    "    df = pd.DataFrame(WomenAllroundData)\n",
    "    df.columns =['Rank', 'Player', 'Team', 'Rating','CareerBestRating']\n",
    "    df['Rank'] = df['Rank'].str.replace('\\n',\"\")\n",
    "    df = df.iloc[1:11]\n",
    "    print(\"Top 10 women’s ODI all-rounder along with the records of their team and rating.\")\n",
    "    display(df)\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    t1 = threading.Thread(target = ODIWomen_10data, args=('https://www.icc-cricket.com/rankings/womens/team-rankings/odi',))\n",
    "    t2 = threading.Thread(target = ODIWomenBatting_10data, args=('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting',))\n",
    "    t3 = threading.Thread(target = ODIWomenAllrounder_10data, args=('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder',))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d47d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>time</th>\n",
       "      <th>newslink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cramer's lightning round: Let Extreme Networks...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jim Cramer says these 3 apparel stocks benefit...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cramer’s week ahead: Markets need a strong job...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There is 'enormous opportunity' in REITs, says...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/reits-offer-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Biden administration will end monkeypox public...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Expect more choppiness ahead after a week of m...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/expect-more-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GM, LG investing $275 million to expand Tennes...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/gm-lg-investin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>These beat-up tech stocks have potential, ‘Hal...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/big-tech-stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Fed's path to a 'Goldilocks' economy just ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/the-feds-path-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3 things crypto investors need to know in post...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/three-things-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Biden condemns antisemitism as Ye praises Hitl...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-condemns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Georgia man arrested for shooting boy campaign...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/georgia-electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What to watch in the markets in the week ahead</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/markets-lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The biggest tax changes to know before filing ...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/the-biggest-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'This is a crisis.' Why more workers need acce...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/why-more-worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Carnival’s Princess Cruises will return to Jap...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/carnivals-prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tech layoffs may not be a bad omen for U.S. ec...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/tech-layoffs-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Professional traders are using these ETFs and ...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/professional-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Amazon media chief Jeff Blackburn retires from...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/amazon-media-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Activision acquisition would be good for Micro...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/activision-acq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Christina Ricci sold off her collection of Cha...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/christina-ricc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Goldman Sachs warns traders of shrinking bonus...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>November unemployment rate fell for both Hispa...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/november-unemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DEI programing stalled in 2022—how that could ...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/dei-programing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Stocks making the biggest moves midday: Zscale...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>People with perfect credit scores share 3 key ...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/experian-peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Student debt relief goes to the Supreme Court—...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/student-debt-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ford claims No. 2 spot in EVs behind Tesla – b...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/ford-claims-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Drew Brees fakes lightning strike in promotion...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/drew-brees-lig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline          time  \\\n",
       "0   Cramer's lightning round: Let Extreme Networks...   4 Hours Ago   \n",
       "1   Jim Cramer says these 3 apparel stocks benefit...   4 Hours Ago   \n",
       "2   Cramer’s week ahead: Markets need a strong job...   5 Hours Ago   \n",
       "3   Pro Picks: Watch all of Friday's big stock cal...   6 Hours Ago   \n",
       "4   There is 'enormous opportunity' in REITs, says...   6 Hours Ago   \n",
       "5   Biden administration will end monkeypox public...   7 Hours Ago   \n",
       "6   Expect more choppiness ahead after a week of m...   7 Hours Ago   \n",
       "7   GM, LG investing $275 million to expand Tennes...   8 Hours Ago   \n",
       "8   These beat-up tech stocks have potential, ‘Hal...   8 Hours Ago   \n",
       "9   The Fed's path to a 'Goldilocks' economy just ...   8 Hours Ago   \n",
       "10  3 things crypto investors need to know in post...   8 Hours Ago   \n",
       "11  Biden condemns antisemitism as Ye praises Hitl...   8 Hours Ago   \n",
       "12  Georgia man arrested for shooting boy campaign...   8 Hours Ago   \n",
       "13     What to watch in the markets in the week ahead   8 Hours Ago   \n",
       "14  The biggest tax changes to know before filing ...   9 Hours Ago   \n",
       "15  'This is a crisis.' Why more workers need acce...   9 Hours Ago   \n",
       "16  Carnival’s Princess Cruises will return to Jap...   9 Hours Ago   \n",
       "17  Tech layoffs may not be a bad omen for U.S. ec...  10 Hours Ago   \n",
       "18  Professional traders are using these ETFs and ...  10 Hours Ago   \n",
       "19  Amazon media chief Jeff Blackburn retires from...  10 Hours Ago   \n",
       "20  Activision acquisition would be good for Micro...  10 Hours Ago   \n",
       "21  Christina Ricci sold off her collection of Cha...  10 Hours Ago   \n",
       "22  Goldman Sachs warns traders of shrinking bonus...  10 Hours Ago   \n",
       "23  November unemployment rate fell for both Hispa...  10 Hours Ago   \n",
       "24  DEI programing stalled in 2022—how that could ...  11 Hours Ago   \n",
       "25  Stocks making the biggest moves midday: Zscale...  11 Hours Ago   \n",
       "26  People with perfect credit scores share 3 key ...  11 Hours Ago   \n",
       "27  Student debt relief goes to the Supreme Court—...  11 Hours Ago   \n",
       "28  Ford claims No. 2 spot in EVs behind Tesla – b...  11 Hours Ago   \n",
       "29  Drew Brees fakes lightning strike in promotion...  11 Hours Ago   \n",
       "\n",
       "                                             newslink  \n",
       "0   https://www.cnbc.com/2022/12/02/cramers-lightn...  \n",
       "1   https://www.cnbc.com/2022/12/02/jim-cramer-say...  \n",
       "2   https://www.cnbc.com/2022/12/02/cramers-week-a...  \n",
       "3   https://www.cnbc.com/2022/12/02/pro-picks-watc...  \n",
       "4   https://www.cnbc.com/2022/12/02/reits-offer-en...  \n",
       "5   https://www.cnbc.com/2022/12/02/biden-administ...  \n",
       "6   https://www.cnbc.com/2022/12/02/expect-more-ch...  \n",
       "7   https://www.cnbc.com/2022/12/02/gm-lg-investin...  \n",
       "8   https://www.cnbc.com/2022/12/02/big-tech-stock...  \n",
       "9   https://www.cnbc.com/2022/12/02/the-feds-path-...  \n",
       "10  https://www.cnbc.com/2022/12/02/three-things-c...  \n",
       "11  https://www.cnbc.com/2022/12/02/biden-condemns...  \n",
       "12  https://www.cnbc.com/2022/12/02/georgia-electi...  \n",
       "13  https://www.cnbc.com/2022/12/02/markets-lookin...  \n",
       "14  https://www.cnbc.com/2022/12/02/the-biggest-ta...  \n",
       "15  https://www.cnbc.com/2022/12/02/why-more-worke...  \n",
       "16  https://www.cnbc.com/2022/12/02/carnivals-prin...  \n",
       "17  https://www.cnbc.com/2022/12/02/tech-layoffs-m...  \n",
       "18  https://www.cnbc.com/2022/12/02/professional-t...  \n",
       "19  https://www.cnbc.com/2022/12/02/amazon-media-c...  \n",
       "20  https://www.cnbc.com/2022/12/02/activision-acq...  \n",
       "21  https://www.cnbc.com/2022/12/02/christina-ricc...  \n",
       "22  https://www.cnbc.com/2022/12/02/goldman-sachs-...  \n",
       "23  https://www.cnbc.com/2022/12/02/november-unemp...  \n",
       "24  https://www.cnbc.com/2022/12/02/dei-programing...  \n",
       "25  https://www.cnbc.com/2022/12/02/stocks-making-...  \n",
       "26  https://www.cnbc.com/2022/12/02/experian-peopl...  \n",
       "27  https://www.cnbc.com/2022/12/02/student-debt-r...  \n",
       "28  https://www.cnbc.com/2022/12/02/ford-claims-no...  \n",
       "29  https://www.cnbc.com/2022/12/02/drew-brees-lig...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "# i)Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def news_data(cnnurl):\n",
    "    url = cnnurl\n",
    "    # creating request object\n",
    "    req = requests.get(url)\n",
    "    # creating soup object\n",
    "    data = BeautifulSoup(req.text, 'html')\n",
    "    # finding all li tags in ul and printing the text within it\n",
    "    data1 = data.find_all('li', {'class':\"LatestNews-item\"})\n",
    "    my_data =[]\n",
    "    for li in data1:\n",
    "            headline = li.a.text\n",
    "            #print(headline)\n",
    "            if len(headline) == 0:\n",
    "                item=li.find('a',class_='LatestNews-headline')\n",
    "                headline=item['title']               \n",
    "            else:\n",
    "                headline= li.a.text  \n",
    "              \n",
    "            time = (li.span.find(class_=\"LatestNews-timestamp\")).text\n",
    "            a_tags = li.find_all('a', href=True) #  Find all <a> tags that have a href attr\n",
    "            #  Loop over the results\n",
    "            for tag in a_tags:\n",
    "                newslink=tag['href']\n",
    "            \n",
    "            my_data.append({\"headline\": headline,\"time\": time, \"newslink\": newslink})\n",
    "\n",
    "    df = pd.DataFrame(my_data)\n",
    "    return df         \n",
    "\n",
    "news_data(\"https://www.cnbc.com/world/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eab6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Author</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>paperURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                               Author   publishedDate  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                             paperURL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "# https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# Scrape below mentioned details :\n",
    "# i) Paper Title \n",
    "# ii) Authors\n",
    "# iii) Published Date \n",
    "# iv) Paper URL\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def AIJournal_data(AIurl):\n",
    "    url = AIurl\n",
    "    # creating request object\n",
    "    req = requests.get(url)\n",
    "    # creating soup object\n",
    "    data = BeautifulSoup(req.text, 'html')\n",
    "    # finding all li tags in ul and printing the text within it\n",
    "    data1 = data.find_all('li', {'class':\"sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs\"})\n",
    "    my_data =[]\n",
    "    for li in data1:\n",
    "        title= li.a.text\n",
    "        #print(title)\n",
    "        Author= li.p.span.text\n",
    "        #print(Author)\n",
    "        publishedDate = (li.find(class_=\"sc-1thf9ly-2 dvggWt\")).text\n",
    "        #print(publishedDate)\n",
    "        a_tags = li.find_all('a', href=True) #  Find all <a> tags that have a href attr\n",
    "        #  Loop over the results\n",
    "        for tag in a_tags:\n",
    "            paperURL=tag['href']\n",
    "            #print(tag['href']) # Print href \n",
    "        my_data.append({\"title\": title, \"Author\": Author,\"publishedDate\": publishedDate, \"paperURL\": paperURL})\n",
    "\n",
    "    df = pd.DataFrame(my_data)\n",
    "    return df          \n",
    "\n",
    "AIJournal_data(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0605ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>location</th>\n",
       "      <th>ratings</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>Continental, Asian, Italian, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local</td>\n",
       "      <td>North Indian, Asian, Continental</td>\n",
       "      <td>Scindia House,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Station Bar</td>\n",
       "      <td>Italian, Chinese, North Indian, Fast Food</td>\n",
       "      <td>F-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QBA</td>\n",
       "      <td>North Indian, Continental, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ministry Of Beer</td>\n",
       "      <td>North Indian, Continental, American, Asian</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unplugged Courtyard</td>\n",
       "      <td>North Indian, Italian, Chinese, Turkish, Conti...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Junkyard Cafe</td>\n",
       "      <td>North Indian, Continental, Chinese, Fast Food</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The G.T. ROAD</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Luggage Room By Sandoz</td>\n",
       "      <td>Chinese, Italian, North Indian, Continental</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chido</td>\n",
       "      <td>North Indian, Italian, Continental, Asian, Fin...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>My Bar Headquarters</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ardor 2.1 Restaurant and Lounge</td>\n",
       "      <td>North Indian, Chinese, Italian, Continental</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sandoz</td>\n",
       "      <td>North Indian, Chinese, Continental</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Connaught Clubhouse Microbrewery</td>\n",
       "      <td>North Indian, Continental, Asian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Out Of The Box Courtyard</td>\n",
       "      <td>North Indian, Mediterranean, Chinese, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Openhouse Cafe</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chili's American Grill and Bar</td>\n",
       "      <td>Mexican, American, Tex Mex</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38 Barracks</td>\n",
       "      <td>North Indian, Chinese, Continental</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Berco's</td>\n",
       "      <td>Chinese, Thai</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cafe Delhi Heights</td>\n",
       "      <td>Continental, North Indian, Beverages, Chinese,...</td>\n",
       "      <td>Janpath, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ghoomar Traditional Thali Restaurant</td>\n",
       "      <td>North Indian, Rajasthani</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Restaurant  \\\n",
       "0                                Tamasha   \n",
       "1                                  Local   \n",
       "2                            Station Bar   \n",
       "3                                    QBA   \n",
       "4                       Ministry Of Beer   \n",
       "5                    Unplugged Courtyard   \n",
       "6                      The Junkyard Cafe   \n",
       "7                          The G.T. ROAD   \n",
       "8             The Luggage Room By Sandoz   \n",
       "9                                  Chido   \n",
       "10                   My Bar Headquarters   \n",
       "11       Ardor 2.1 Restaurant and Lounge   \n",
       "12                                Sandoz   \n",
       "13      Connaught Clubhouse Microbrewery   \n",
       "14              Out Of The Box Courtyard   \n",
       "15                        Openhouse Cafe   \n",
       "16        Chili's American Grill and Bar   \n",
       "17                           38 Barracks   \n",
       "18                               Berco's   \n",
       "19                    Cafe Delhi Heights   \n",
       "20  Ghoomar Traditional Thali Restaurant   \n",
       "\n",
       "                                              Cuisine  \\\n",
       "0           Continental, Asian, Italian, North Indian   \n",
       "1                    North Indian, Asian, Continental   \n",
       "2           Italian, Chinese, North Indian, Fast Food   \n",
       "3                  North Indian, Continental, Italian   \n",
       "4          North Indian, Continental, American, Asian   \n",
       "5   North Indian, Italian, Chinese, Turkish, Conti...   \n",
       "6       North Indian, Continental, Chinese, Fast Food   \n",
       "7                                        North Indian   \n",
       "8         Chinese, Italian, North Indian, Continental   \n",
       "9   North Indian, Italian, Continental, Asian, Fin...   \n",
       "10                              North Indian, Chinese   \n",
       "11        North Indian, Chinese, Italian, Continental   \n",
       "12                 North Indian, Chinese, Continental   \n",
       "13          North Indian, Continental, Asian, Chinese   \n",
       "14      North Indian, Mediterranean, Chinese, Italian   \n",
       "15                       North Indian, Asian, Italian   \n",
       "16                         Mexican, American, Tex Mex   \n",
       "17                 North Indian, Chinese, Continental   \n",
       "18                                      Chinese, Thai   \n",
       "19  Continental, North Indian, Beverages, Chinese,...   \n",
       "20                           North Indian, Rajasthani   \n",
       "\n",
       "                                        location ratings  \\\n",
       "0                 Connaught Place, Central Delhi     4.2   \n",
       "1   Scindia House,Connaught Place, Central Delhi       4   \n",
       "2         F-Block,Connaught Place, Central Delhi       4   \n",
       "3                 Connaught Place, Central Delhi     4.3   \n",
       "4         M-Block,Connaught Place, Central Delhi       4   \n",
       "5                 Connaught Place, Central Delhi       4   \n",
       "6                 Connaught Place, Central Delhi     4.1   \n",
       "7         M-Block,Connaught Place, Central Delhi     4.3   \n",
       "8         M-Block,Connaught Place, Central Delhi     3.9   \n",
       "9                 Connaught Place, Central Delhi     4.2   \n",
       "10                Connaught Place, Central Delhi       4   \n",
       "11                Connaught Place, Central Delhi     3.8   \n",
       "12                Connaught Place, Central Delhi       4   \n",
       "13                Connaught Place, Central Delhi     4.3   \n",
       "14                Connaught Place, Central Delhi     4.1   \n",
       "15                Connaught Place, Central Delhi     4.1   \n",
       "16        M-Block,Connaught Place, Central Delhi     4.3   \n",
       "17        M-Block,Connaught Place, Central Delhi     4.3   \n",
       "18                Connaught Place, Central Delhi     4.3   \n",
       "19                        Janpath, Central Delhi     4.3   \n",
       "20                Connaught Place, Central Delhi     4.1   \n",
       "\n",
       "                                               images  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in :\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location \n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def dineout_data(dineurl):\n",
    "    url = dineurl\n",
    "    data = requests.get(url)\n",
    "    html = BeautifulSoup(data.text, 'html.parser')\n",
    "    ratings =[]\n",
    "    data11 = html.find_all('div', {'class':\"restnt-rating rating-4\"})\n",
    "    for each in data11:\n",
    "        rating = each.text\n",
    "        ratings.append(rating)\n",
    "    #print(ratings)\n",
    "    images = []\n",
    "    for img in html.find_all(\"img\",class_=\"no-img\"):\n",
    "           images.append(img.get('data-src'))\n",
    "    #print(images)       \n",
    "    res_list = []\n",
    "    cuisine_list = []\n",
    "    location_list = []\n",
    "    dineall = html.find_all('div', {'class':\"restnt-card restaurant\"})\n",
    "    \n",
    "    for dine in dineall:\n",
    "        Restaurant = dine.find(class_=\"restnt-name ellipsis\").get_text()\n",
    "        #print(Restaurant)\n",
    "        Cuisine = (dine.find(class_=\"double-line-ellipsis\").get_text()).split('|')[1].strip()\n",
    "        #print(Cuisine.split('|')[1].strip())\n",
    "        location = dine.find(class_=\"restnt-loc ellipsis\").get_text()\n",
    "        #print(location)\n",
    "        res_list.append(Restaurant)\n",
    "        cuisine_list.append(Cuisine)\n",
    "        location_list.append(location)\n",
    "\n",
    "    res=pd.Series(res_list,name='Restaurant')\n",
    "    cuisine=pd.Series(cuisine_list,name='Cuisine')\n",
    "    location=pd.Series(location_list,name='location')\n",
    "    ratings=pd.Series(ratings,name='ratings')\n",
    "    images=pd.Series(images,name='images')\n",
    "    dine_list = pd.concat([res,cuisine,location,ratings,images], axis=1)\n",
    "    return dine_list        \n",
    "\n",
    "dineout_data(\"https://www.dineout.co.in/delhi-restaurants/welcome-back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8aeee7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                                        Publication h5-index  \\\n",
       "1      1.                                             Nature      444   \n",
       "2      2.                The New England Journal of Medicine      432   \n",
       "3      3.                                            Science      401   \n",
       "4      4.  IEEE/CVF Conference on Computer Vision and Pat...      389   \n",
       "5      5.                                         The Lancet      354   \n",
       "..    ...                                                ...      ...   \n",
       "96    96.                       Journal of Business Research      145   \n",
       "97    97.                                   Molecular Cancer      145   \n",
       "98    98.                                            Sensors      145   \n",
       "99    99.                              Nature Climate Change      144   \n",
       "100  100.                    IEEE Internet of Things Journal      144   \n",
       "\n",
       "    h5-median  \n",
       "1         667  \n",
       "2         780  \n",
       "3         614  \n",
       "4         627  \n",
       "5         635  \n",
       "..        ...  \n",
       "96        233  \n",
       "97        209  \n",
       "98        201  \n",
       "99        228  \n",
       "100       212  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape the details of top publications from Google Scholar from \n",
    "# https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "# i) Rank \n",
    "# ii) Publication\n",
    "# iii) h5-index\n",
    "#  iv) h5-median\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def google_data(googleurl):\n",
    "    url = googleurl\n",
    "    data = requests.get(url).text\n",
    "    bs=BeautifulSoup(data, \"lxml\")\n",
    "    rows = bs.find_all('tr')\n",
    "    contents = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        #print(cols)\n",
    "        contents.append(cols)\n",
    "    df = pd.DataFrame(contents)\n",
    "    df.columns =['Rank', 'Publication', 'h5-index', 'h5-median']\n",
    "    df = df.iloc[1: , :]\n",
    "    return df\n",
    "\n",
    "google_data(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
